{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn68MeRdwXMuWWhKjQN2JI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetdave/LangChainTutorials/blob/main/AdvancedLSTM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P_tNXKCPMid6"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import Libraries\n",
        "# We're adding new tools for a more robust workflow\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam  # Import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=Warning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define Sample Data (Now with more text!)\n",
        "# --- DATASET SIGNIFICANTLY EXPANDED ---\n",
        "# The model needs much more data to learn real patterns.\n",
        "data = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do: once or twice she had peeped into the\n",
        "book her sister was reading, but it had no pictures or conversations in\n",
        "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
        "conversations?' So she was considering in her own mind (as well as she\n",
        "could, for the hot day made her feel very sleepy and stupid), whether\n",
        "the pleasure of making a daisy-chain would be worth the trouble of\n",
        "getting up and picking the daisies, when suddenly a White Rabbit with\n",
        "pink eyes ran close by her.\n",
        "\n",
        "There was nothing so very remarkable in that; nor did Alice\n",
        "think it so very much out of the way to hear the Rabbit say to\n",
        "itself, 'Oh dear! Oh dear! I shall be late!' (when she thought\n",
        "it over afterwards, it occurred to her that she ought to have\n",
        "wondered at this, but at the time it all seemed quite natural);\n",
        "but when the Rabbit actually took a watch out of its waistcoat-pocket,\n",
        "and looked at it, and then hurried on, Alice started to her feet,\n",
        "for it flashed across her mind that she had never before seen a\n",
        "rabbit with either a waistcoat-pocket, or a watch to take out of it,\n",
        "and burning with curiosity, she ran across the field after it,\n",
        "and fortunately was just in time to see it pop down a large\n",
        "rabbit-hole under the hedge.\n",
        "\n",
        "In another moment down went Alice after it, never once considering how\n",
        "in the world she was to get out again.\n",
        "\n",
        "The rabbit-hole went straight on like a tunnel for some way, and then\n",
        "dipped suddenly down, so suddenly that Alice had not a moment to\n",
        "think about stopping herself before she found herself falling down\n",
        "what seemed to be a very deep well.\n",
        "\n",
        "Either the well was very deep, or she fell very slowly, for she\n",
        "had plenty of time as she went down to look about her and to wonder\n",
        "what was going to happen next. First, she tried to look down and\n",
        "make out what she was coming to, but it was too dark to see anything;\n",
        "then she looked at the sides of the well, and noticed that they were\n",
        "filled with cupboards and book-shelves; here andD there she saw maps\n",
        "and pictures hung upon pegs. She took down a jar from one of the\n",
        "shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her\n",
        "great disappointment it was empty: she did not like to drop the jar\n",
        "for fear of killing somebody underneath, so managed to put it into\n",
        "one of the cupboards as she fell past it.\n",
        "\n",
        "'Well!' thought Alice to herself. 'After such a fall as this,\n",
        "I shall think nothing of tumbling down stairs! How brave they'll\n",
        "all think me at home! Why, I wouldn't say anything about it,\n",
        "even if I fell off the top of the house!' (Which was very likely true.)\n",
        "\n",
        "Down, down, down. Would the fall never come to an end? 'I wonder\n",
        "how many miles I've fallen by this time?' she said aloud. 'I must\n",
        "be getting somewhere near the centre of the earth. Let me see:\n",
        "that would be four thousand miles down, I think–' (for, you\n",
        "see, Alice had learnt several things of this sort in her lessons\n",
        "in the schoolroom, and though this was not a very good opportunity\n",
        "for showing off her knowledge, as there was no one to listen to\n",
        "her, still it was good practice to say it over) '–yes, that's\n",
        "about the right distance–but then I wonder what Latitude or\n",
        "Longitude I've got to?' (Alice had no idea what Latitude was,\n",
        "or Longitude either, but thought they were nice grand words to say.)\n",
        "\"\"\"\n",
        "\n",
        "print(\"Data loaded (now with more text!).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNYQr51_MkSm",
        "outputId": "1920a5d7-0107-4c07-e259-3cd91ded159c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded (now with more text!).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 3: Tokenize the Text\n",
        "# This step is the same, but the tokenizer will now find more unique words.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoded_text = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "print(f\"Total unique words (vocab size): {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqp2FWavMqBe",
        "outputId": "febc4033-72f7-4eb2-ecfa-aad77d8f8b31"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words (vocab size): 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 4: Create Input Sequences and Targets (Longer Sequence)\n",
        "# We're increasing the seq_length to 10. This gives the model\n",
        "# more context to learn from.\n",
        "sequences = []\n",
        "seq_length = 10  # Increased from 5 to 10\n",
        "\n",
        "for i in range(seq_length, len(encoded_text)):\n",
        "    seq = encoded_text[i-seq_length:i]\n",
        "    label = encoded_text[i]\n",
        "    sequences.append((seq, label))\n",
        "\n",
        "print(f\"Total number of sequences created: {len(sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFFEBhM3M0EH",
        "outputId": "33323775-18e5-4c73-f8c8-b3b129821bee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sequences created: 641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 5: Prepare Data for Keras (with Validation Split)\n",
        "# This is a major improvement. We split our data into a training set\n",
        "# (for learning) and a validation set (for testing). This lets us\n",
        "# see if the model is just memorizing or actually learning.\n",
        "\n",
        "X, y = zip(*sequences)\n",
        "X = np.array(X)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Split the data: 80% for training, 20% for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train (inputs): {X_train.shape}\")\n",
        "print(f\"Shape of y_train (targets): {y_train.shape}\")\n",
        "print(f\"Shape of X_val (validation inputs): {X_val.shape}\")\n",
        "print(f\"Shape of y_val (validation targets): {y_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Y3jaO8M1ce",
        "outputId": "f781aefb-d5b1-4794-ae6e-0b3acbe8b131"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train (inputs): (512, 10)\n",
            "Shape of y_train (targets): (512, 271)\n",
            "Shape of X_val (validation inputs): (129, 10)\n",
            "Shape of y_val (validation targets): (129, 271)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 6: Define the Improved Model Architecture\n",
        "# --- MODEL SIMPLIFIED EVEN FURTHER TO PREVENT OVERFITTING ---\n",
        "embedding_dim = 32  # Reduced dimensions\n",
        "lstm_units = 64     # Reduced memory units\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    input_length=seq_length))\n",
        "# Increased dropout\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Layer 1: A single, smaller LSTM layer\n",
        "model.add(LSTM(lstm_units))\n",
        "# Increased dropout\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer (same as before)\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "o4LgMF0qM7lI",
        "outputId": "106b5cb0-22a2-47ac-c756-f94066b4fe58"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 7: Compile and Train the Model (with Early Stopping)\n",
        "# --- ADJUSTED LEARNING RATE AND PATIENCE ---\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),  # Using Adam with a slightly smaller rate\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Increased patience to 15\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Starting enhanced model training...\")\n",
        "# We set epochs to a reasonable number. EarlyStopping will\n",
        "# likely stop it before this number is reached.\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val),  # Pass in the validation set\n",
        "    callbacks=[early_stopper],       # Add the early stopping callback\n",
        "    verbose=2\n",
        ")\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOis7b4ZNQ0O",
        "outputId": "6841966f-ddf7-4666-9bb3-a761fa7bb2ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting enhanced model training...\n",
            "Epoch 1/100\n",
            "64/64 - 4s - 60ms/step - accuracy: 0.0391 - loss: 5.5839 - val_accuracy: 0.0388 - val_loss: 5.5029\n",
            "Epoch 2/100\n",
            "64/64 - 0s - 8ms/step - accuracy: 0.0312 - loss: 5.2360 - val_accuracy: 0.0388 - val_loss: 5.5122\n",
            "Epoch 3/100\n",
            "64/64 - 1s - 9ms/step - accuracy: 0.0430 - loss: 5.0427 - val_accuracy: 0.0620 - val_loss: 5.6144\n",
            "Epoch 4/100\n",
            "64/64 - 1s - 9ms/step - accuracy: 0.0449 - loss: 4.9716 - val_accuracy: 0.0388 - val_loss: 5.9650\n",
            "Epoch 5/100\n",
            "64/64 - 1s - 13ms/step - accuracy: 0.0371 - loss: 4.9337 - val_accuracy: 0.0543 - val_loss: 6.0319\n",
            "Epoch 6/100\n",
            "64/64 - 1s - 13ms/step - accuracy: 0.0371 - loss: 4.9060 - val_accuracy: 0.0543 - val_loss: 6.0669\n",
            "Epoch 7/100\n",
            "64/64 - 1s - 13ms/step - accuracy: 0.0488 - loss: 4.8611 - val_accuracy: 0.0388 - val_loss: 6.1615\n",
            "Epoch 8/100\n",
            "64/64 - 1s - 8ms/step - accuracy: 0.0430 - loss: 4.8208 - val_accuracy: 0.0388 - val_loss: 6.0934\n",
            "Epoch 9/100\n",
            "64/64 - 1s - 10ms/step - accuracy: 0.0352 - loss: 4.7955 - val_accuracy: 0.0388 - val_loss: 6.0005\n",
            "Epoch 10/100\n",
            "64/64 - 1s - 8ms/step - accuracy: 0.0488 - loss: 4.7371 - val_accuracy: 0.0233 - val_loss: 5.9152\n",
            "Epoch 11/100\n",
            "64/64 - 1s - 8ms/step - accuracy: 0.0430 - loss: 4.6548 - val_accuracy: 0.0388 - val_loss: 5.9546\n",
            "Epoch 12/100\n",
            "64/64 - 0s - 8ms/step - accuracy: 0.0508 - loss: 4.6217 - val_accuracy: 0.0543 - val_loss: 6.1570\n",
            "Epoch 13/100\n",
            "64/64 - 1s - 10ms/step - accuracy: 0.0547 - loss: 4.5262 - val_accuracy: 0.0465 - val_loss: 6.3487\n",
            "Epoch 14/100\n",
            "64/64 - 0s - 8ms/step - accuracy: 0.0586 - loss: 4.4680 - val_accuracy: 0.0310 - val_loss: 6.4220\n",
            "Epoch 15/100\n",
            "64/64 - 1s - 8ms/step - accuracy: 0.0410 - loss: 4.3892 - val_accuracy: 0.0155 - val_loss: 6.4695\n",
            "Epoch 16/100\n",
            "64/64 - 1s - 8ms/step - accuracy: 0.0762 - loss: 4.3384 - val_accuracy: 0.0310 - val_loss: 6.6083\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 8: Define the Text Generation Function\n",
        "# --- REPLACED with a much better function that uses Temperature Sampling ---\n",
        "# This prevents the model from getting stuck in loops (\"the the the\")\n",
        "# and allows for more creative, varied text generation.\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Helper function to sample an index from a probability array,\n",
        "    controlled by temperature.\n",
        "    A higher temperature (e.g., 1.2) = more random, creative text.\n",
        "    A lower temperature (e.g., 0.5) = more conservative, predictable text.\n",
        "    \"\"\"\n",
        "    # Clip predictions to avoid log(0)\n",
        "    preds = np.clip(preds, 1e-7, 1 - 1e-7)\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_text(seed_text, n_words, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generates 'n_words' of text based on a 'seed_text' and a 'temperature'.\n",
        "    \"\"\"\n",
        "    generated_text = seed_text\n",
        "    current_text = seed_text\n",
        "\n",
        "    int_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        # Tokenize the *current* text seed\n",
        "        encoded = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        # Pad the sequence from the beginning\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\n",
        "        y_pred_probs = model.predict(encoded, verbose=0)[0]\n",
        "\n",
        "        # Use the 'sample' function instead of np.argmax\n",
        "        y_pred_index = sample(y_pred_probs, temperature)\n",
        "\n",
        "        out_word = int_to_word.get(y_pred_index, '?')\n",
        "\n",
        "        # Append the new word to build the next sequence\n",
        "        current_text += \" \" + out_word\n",
        "        generated_text += \" \" + out_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"Text generation function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWizOvC4NVzZ",
        "outputId": "912ea6bf-2431-4b97-ad9d-d9cbf600983d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 9: Generate New Text\n",
        "# Now we test our new, more robust model.\n",
        "\n",
        "seed_text = \"alice was beginning to get\"\n",
        "generated = generate_text(seed_text, 30, temperature=0.8) # Generate 30 new words\n",
        "\n",
        "print(\"\\n--- SEED TEXT ---\")\n",
        "print(seed_text)\n",
        "print(\"\\n--- GENERATED TEXT (from Enhanced LSTM) ---\")\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGW5rZUvNaHZ",
        "outputId": "8c23a144-6939-4540-b0ff-696ce84a25eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SEED TEXT ---\n",
            "alice was beginning to get\n",
            "\n",
            "--- GENERATED TEXT (from Enhanced LSTM) ---\n",
            "alice was beginning to get straight true fall she one maps what took this one either one down quite opportunity filled is several hurried getting put straight even out nor maps by for fall in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 10: Inspect the LSTM Layer Weights\n",
        "# This cell extracts the trained weights from the LSTM layer.\n",
        "# It should be run AFTER Cell 7 has completed training.\n",
        "\n",
        "print(\"\\n--- Inspecting LSTM Layer Weights ---\")\n",
        "\n",
        "# The LSTM layer is the 3rd layer in our model (index 2)\n",
        "# [0] = Embedding\n",
        "# [1] = Dropout\n",
        "# [2] = LSTM\n",
        "# [3] = Dropout\n",
        "# [4] = Dense\n",
        "try:\n",
        "    lstm_layer = model.layers[2]\n",
        "\n",
        "    # get_weights() returns a list of 3 NumPy arrays:\n",
        "    # [0] = Input Kernel (W) - for the input word embeddings\n",
        "    # [1] = Recurrent Kernel (U) - for the previous hidden state\n",
        "    # [2] = Biases (b)\n",
        "    lstm_weights = lstm_layer.get_weights()\n",
        "\n",
        "    W_input = lstm_weights[0]\n",
        "    W_recurrent = lstm_weights[1]\n",
        "    b_all = lstm_weights[2]\n",
        "\n",
        "    print(f\"Total weight arrays found: {len(lstm_weights)}\")\n",
        "    print(f\"Shape of Input Kernel (W_input): {W_input.shape}\")\n",
        "    print(f\"Shape of Recurrent Kernel (W_recurrent): {W_recurrent.shape}\")\n",
        "    print(f\"Shape of Biases (b_all): {b_all.shape}\")\n",
        "\n",
        "    # --- Now, let's break them down ---\n",
        "\n",
        "    # The Keras LSTM concatenates the weights for its 4 internal gates\n",
        "    # (Input, Forget, Candidate, Output) into one giant matrix.\n",
        "    # The total number of columns is 4 * lstm_units (4 * 32 = 128)\n",
        "\n",
        "    print(\"\\n--- Explanation ---\")\n",
        "    print(f\"The Input Kernel shape is (embedding_dim, 4 * lstm_units) -> ({embedding_dim}, {4 * lstm_units})\")\n",
        "    print(f\"The Recurrent Kernel shape is (lstm_units, 4 * lstm_units) -> ({lstm_units}, {4 * lstm_units})\")\n",
        "    print(f\"The Bias shape is (4 * lstm_units,) -> ({4 * lstm_units},)\")\n",
        "\n",
        "    # We can \"slice\" these matrices to see the weights for a single gate.\n",
        "    # The standard Keras order is: Input Gate, Forget Gate, Candidate Gate, Output Gate.\n",
        "\n",
        "    # Weights for the Forget Gate (the second chunk of size lstm_units)\n",
        "    start = lstm_units\n",
        "    end = 2 * lstm_units\n",
        "\n",
        "    W_input_forget_gate = W_input[:, start:end]\n",
        "    W_recurrent_forget_gate = W_recurrent[:, start:end]\n",
        "    b_forget_gate = b_all[start:end]\n",
        "\n",
        "    print(\"\\n--- Example: Sliced Weights for the 'Forget Gate' ---\")\n",
        "    print(f\"Input Kernel (Forget Gate) shape: {W_input_forget_gate.shape}\")\n",
        "    print(f\"Recurrent Kernel (Forget Gate) shape: {W_recurrent_forget_gate.shape}\")\n",
        "    print(f\"Bias (Forget Gate) shape: {b_forget_gate.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred. Did you run Cell 7 to train the model first?\")\n",
        "    print(f\"Error details: {e}\")"
      ],
      "metadata": {
        "id": "zbXSIeV2jNlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}