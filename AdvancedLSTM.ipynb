{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoLdsaRJ5sX9icNizGe8hG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetdave/LangChainTutorials/blob/main/AdvancedLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiSkU8C4uIxH"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import Libraries\n",
        "# We're adding new tools for a more robust workflow\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=Warning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 2: Define Sample Data (Now with more text!)\n",
        "# A model needs more data to learn patterns, so we've replaced\n",
        "# the tiny story with a larger excerpt.\n",
        "data = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do: once or twice she had peeped into the\n",
        "book her sister was reading, but it had no pictures or conversations in\n",
        "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
        "conversations?' So she was considering in her own mind (as well as she\n",
        "could, for the hot day made her feel very sleepy and stupid), whether\n",
        "the pleasure of making a daisy-chain would be worth the trouble of\n",
        "getting up and picking the daisies, when suddenly a White Rabbit with\n",
        "pink eyes ran close by her.\n",
        "\n",
        "There was nothing so very remarkable in that; nor did Alice\n",
        "think it so very much out of the way to hear the Rabbit say to\n",
        "itself, 'Oh dear! Oh dear! I shall be late!' (when she thought\n",
        "it over afterwards, it occurred to her that she ought to have\n",
        "wondered at this, but at the time it all seemed quite natural);\n",
        "but when the Rabbit actually took a watch out of its waistcoat-pocket,\n",
        "and looked at it, and then hurried on, Alice started to her feet,\n",
        "for it flashed across her mind that she had never before seen a\n",
        "rabbit with either a waistcoat-pocket, or a watch to take out of it,\n",
        "and burning with curiosity, she ran across the field after it,\n",
        "and fortunately was just in time to see it pop down a large\n",
        "rabbit-hole under the hedge.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Data loaded (now with more text!).\")"
      ],
      "metadata": {
        "id": "Cut5sRMUujul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 3: Tokenize the Text\n",
        "# This step is the same, but the tokenizer will now find more unique words.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoded_text = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "print(f\"Total unique words (vocab size): {vocab_size}\")"
      ],
      "metadata": {
        "id": "CDAY-5ubukyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 4: Create Input Sequences and Targets (Longer Sequence)\n",
        "# We're increasing the seq_length to 10. This gives the model\n",
        "# more context to learn from.\n",
        "sequences = []\n",
        "seq_length = 10  # Increased from 5 to 10\n",
        "\n",
        "for i in range(seq_length, len(encoded_text)):\n",
        "    seq = encoded_text[i-seq_length:i]\n",
        "    label = encoded_text[i]\n",
        "    sequences.append((seq, label))\n",
        "\n",
        "print(f\"Total number of sequences created: {len(sequences)}\")"
      ],
      "metadata": {
        "id": "NnFv_t9RuqtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 5: Prepare Data for Keras (with Validation Split)\n",
        "# This is a major improvement. We split our data into a training set\n",
        "# (for learning) and a validation set (for testing). This lets us\n",
        "# see if the model is just memorizing or actually learning.\n",
        "\n",
        "X, y = zip(*sequences)\n",
        "X = np.array(X)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Split the data: 80% for training, 20% for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train (inputs): {X_train.shape}\")\n",
        "print(f\"Shape of y_train (targets): {y_train.shape}\")\n",
        "print(f\"Shape of X_val (validation inputs): {X_val.shape}\")\n",
        "print(f\"Shape of y_val (validation targets): {y_val.shape}\")"
      ],
      "metadata": {
        "id": "p8sLA8zjutmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 6: Define the Improved Model Architecture\n",
        "# We are now building a deeper, more robust model by:\n",
        "# 1. Stacking two LSTM layers.\n",
        "# 2. Adding Dropout layers to prevent overfitting.\n",
        "\n",
        "embedding_dim = 100  # Increased dimensions for a richer representation\n",
        "lstm_units = 150     # Increased memory units\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    input_length=seq_length))\n",
        "# Dropout Layer: Randomly \"turns off\" 20% of neurons to prevent memorization\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Layer 1: Stacked LSTM\n",
        "# We set return_sequences=True so it passes its full output sequence\n",
        "# to the next LSTM layer, not just the final summary.\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Layer 2: Final LSTM layer\n",
        "# This one returns only the final summary vector.\n",
        "model.add(LSTM(lstm_units))\n",
        "\n",
        "# Output Layer (same as before, but now fed by a more powerful model)\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JIkeHH4iuyZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Cell 7: Compile and Train the Model (with Early Stopping)\n",
        "# We add two major improvements here:\n",
        "# 1. validation_data: The model will test itself against the unseen\n",
        "#    validation set after each epoch.\n",
        "# 2. callbacks: We add EarlyStopping to automatically stop training\n",
        "#    when the model's validation score stops improving.\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# This callback will monitor the 'val_loss' (validation loss).\n",
        "# It will stop training if the val_loss doesn't improve for 10 epochs (patience=10).\n",
        "# It will also restore the best version of the model it found.\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Starting enhanced model training...\")\n",
        "# We can set epochs high (e.g., 500) because EarlyStopping will find\n",
        "# the \"best\" epoch and stop automatically.\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=500,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val),  # Pass in the validation set\n",
        "    callbacks=[early_stopper],       # Add the early stopping callback\n",
        "    verbose=2\n",
        ")\n",
        "print(\"Model training complete.\")\n"
      ],
      "metadata": {
        "id": "RtX9T1MZu6wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 8: Define the Text Generation Function\n",
        "# This function is identical to the one before.\n",
        "def generate_text(seed_text, n_words):\n",
        "    generated_text = seed_text\n",
        "    current_text = seed_text\n",
        "\n",
        "    int_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        encoded = tokenizer.texts_to_sequences([current_text])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\n",
        "        y_pred_probs = model.predict(encoded, verbose=0)[0]\n",
        "        y_pred_index = np.argmax(y_pred_probs)\n",
        "\n",
        "        out_word = int_to_word.get(y_pred_index, '?')\n",
        "\n",
        "        current_text += \" \" + out_word\n",
        "        generated_text += \" \" + out_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"Text generation function defined.\")"
      ],
      "metadata": {
        "id": "O6s_C7awu-uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Cell 9: Generate New Text\n",
        "# Now we test our new, more robust model. The generated text\n",
        "# should (hopefully) be more coherent.\n",
        "\n",
        "seed_text = \"the knight was very brave and\"\n",
        "generated = generate_text(seed_text, 30) # Generate 30 new words\n",
        "\n",
        "print(\"\\n--- SEED TEXT ---\")\n",
        "print(seed_text)\n",
        "print(\"\\n--- GENERATED TEXT (from Enhanced LSTM) ---\")\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "PtKkcjnxvCV8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}